;
; Sample ceph ceph.conf file.
;
; This file defines cluster membership, the various locations
; that Ceph stores data, and any other runtime options.

; If a 'host' is defined for a daemon, the init.d start/stop script will
; verify that it matches the hostname (or else ignore it).  If it is
; not defined, it is assumed that the daemon is intended to start on
; the current host (e.g., in a setup with a startup.conf on each
; node).

; The variables $type, $id and $name are available to use in paths
; $type = The type of daemon, possible values: mon, mds and osd
; $id = The ID of the daemon, for mon.alpha, $id will be alpha
; $name = $type.$id

; For example:
; osd.0
;  $type = osd
;  $id = 0
;  $name = osd.0

; mon.beta
;  $type = mon
;  $id = beta
;  $name = mon.beta

; global
[global]
        osd pool default size = 3
        osd pool default min size = 1
        ; Ensure you have a realistic number of placement groups. We recommend 
	; approximately 100 per OSD. E.g., total number of OSDs multiplied by 100 
	; divided by the number of replicas (i.e., osd pool default size). So for
	; 10 OSDs and osd pool default size = 3, we'd recommend approximately 
	; (100 * 10) / 3 = 333.
	osd pool default pg num = 400 
	osd pool default pgp num = 400
	; enable secure authentication
	auth supported = cephx

        ; allow ourselves to open a lot of files
        max open files = 320000 

        ; set log file
        log file = /var/log/ceph/$name.log
        log_to_syslog = false        ; uncomment this line to log to syslog

        ; set up pid files
        pid file = /var/run/ceph/$name.pid
       
        ; If you want to run a IPv6 cluster, set this to true. Dual-stack isn't possible
        ;ms bind ipv6 = true
	
	; randy, 11.28
	filestore_xattr_use_omap = true
; monitors
;  You need at least one.  You need at least three if you want to
;  tolerate any node failures.  Always create an odd number.
[mon]
	mon initial members = a,b,c
        mon data = /var/lib/ceph/mon/$name
        ; If you are using for example the RADOS Gateway and want to have your newly created
        ; pools a higher replication level, you can set a default
        ;osd pool default size = 3

        ; You can also specify a CRUSH rule for new pools
        ; Wiki: http://ceph.newdream.net/wiki/Custom_data_placement_with_CRUSH
        ;osd pool default crush rule = 0

        ; Timing is critical for monitors, but if you want to allow the clocks to drift a
        ; bit more, you can specify the max drift.
        ;mon clock drift allowed = 1

        ; Tell the monitor to backoff from this warning for 30 seconds
        ;mon clock drift warn backoff = 30

	; logging, for debugging monitor crashes, in order of
	; their likelihood of being helpful :)
	 debug ms = 20
	 debug mon = 20
	 debug paxos = 1
	 debug auth = 20

; updated by randy
[mon.a]
	host = osd-0 
	mon addr = 192.168.8.17:6789
[mon.b]
	host = osd-1 
	mon addr = 192.168.8.18:6789
[mon.c]
	host = osd-2 
	mon addr = 192.168.8.19:6789
; mds
;  You need at least one.  Define two to get a standby.
[mds]
	; where the mds keeps it's secret encryption keys
	; keyring = /usr/local/cephdata/keyring.$name

	; mds logging to debug issues.
	;debug ms = 1
	;debug mds = 20

[mds.alpha]
	host = 192.168.8.17 
        mds addr = 192.168.8.17 

; osd
;  You need at least one.  Two if you want data to be replicated.
;  Define as many as you like.
[osd]
	; This is where the osd expects its data

	; Ideally, make the journal a separate disk or partition.
 	; 1-10GB should be enough; more if you have fast or many
 	; disks.  You can use a file under the osd data dir if need be
 	; (e.g. /data/$name/journal), but it will be slower than a
 	; separate disk or partition.
        ; This is an example of a file-based journal.
	osd journal = /var/log/ceph/$name/journal
	osd journal size = 1000 ; journal size, in megabytes

        ; If you want to run the journal on a tmpfs (don't), disable DirectIO
        ;journal dio = false

        ; You can change the number of recovery operations to speed up recovery
        ; or slow it down if your machines can't handle it
        ; osd recovery max active = 3
	osd_recovery_threads = 2
	; osd logging to debug osd issues, in order of likelihood of being
	; helpful
	debug ms = 20
	debug osd = 20
	debug filestore = 1 
	debug journal = 1 


	; ### The below options only apply if you're using mkcephfs
	; ### and the devs options
        ; The filesystem used on the volumes
        ; not use mkcephfs
        ; If you want to specify some other mount options, you can do so.
        ; for other filesystems use 'osd mount options $fstype'
	;osd mount options btrfs = rw,noatime
	; The options used to format the filesystem via mkfs.$fstype
        ; for other filesystems use 'osd mkfs options $fstype'
	; osd mkfs options btrfs =
        osd mkfs type = xfs
	osd mount options xfs = rw,noatime,inode64,logbsize=256k,delaylog
	osd mkfs options xfs = -f -i size=2048

	; add by randy 11.28
        osd op threads = 8
	osd disk threads = 8
	 
        ; osd op thread time out; how  to set? rely on object size
        ; osd op thread time out = 15
        ; osd_op_complaint_time = 30
        osd op history size = 500
      
        ;filestore config
        filestore queue max ops = 5000
	filestore queue max bytes = 1048576000
	filestore queue committing max bytes = 1048576000
	filestore queue committing max ops = 5000
	filestore op threads = 8
	filestore journal writeahead = true

	;filesytem config
	filestore_wbthrottle_xfs_ios_start_flusher = 10000000
 	filestore_wbthrottle_xfs_ios_hard_limit = 10000000
  	filestore_wbthrottle_xfs_inodes_start_flusher = 10000
  	filestore_wbthrottle_xfs_inodes_hard_limit = 10000

	; randy 11.28
	journal max write entries = 1000
	journal queue max ops = 5000
	objecter inflight ops = 8192
	journal max write bytes = 1048576000
	journal queue max bytes = 1048576000
	ms dispatch throttle bytes = 1048576000
	objecter inflight op bytes = 1048576000

	osd_client_message_cap = 1000000
[osd.0]
	host = osd-0 
        osd addr = 192.168.8.17
	devs = /dev/sdb
[osd.1]
	host = osd-0 
        osd addr = 192.168.8.17
	devs = /dev/sdc
[osd.2]
	host = osd-0 
        osd addr = 192.168.8.17
	devs = /dev/sdd
[osd.3]
	host = osd-1
        osd addr = 192.168.8.18
	devs = /dev/sdb
[osd.4]
	host = osd-1
        osd addr = 192.168.8.18
	devs = /dev/sdc
[osd.6]
	host = osd-2
        osd addr = 192.168.8.19
	devs = /dev/sdb
[osd.7]
	host = osd-2
        osd addr = 192.168.8.19
	devs = /dev/sdc
[osd.9]
	host = admin
        osd addr = 192.168.8.12
	devs = /dev/sdb
[osd.10]
	host = admin
        osd addr = 192.168.8.12
	devs = /dev/sdc
[osd.11]
	host = admin 
        osd addr = 192.168.8.12
	devs = /dev/sdd

[client]
        rbd cache = true
        rbd cache size = 512M
        rbd cache max dirty = 128M
        rbd cache target dirty = 64M
        rbd cache max dirty age = 5

[client.images]
	keyring = /etc/ceph/images.keyring
[client.volumes]
	keyring = /etc/ceph/volumes.keyring
[client.cinder]
 	keyring = /etc/ceph/cinder.keyring
[client.admin]
	keyring = /etc/ceph/keyring
